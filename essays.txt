Naoki Sakai wrote Voices of the Past in 1991. In the book, he examines the role of language in eighteenth century Japanese discourse. Chapter 3, titled "Textuality and Sociality: the Question of Praxis, Exteriority, and the Split in Enunciation," explains the universality of nature and feeling through the unconventional perspective of Ito Jinsai, then explain how we use the fluctuating nature of feeling to interact with each other in combination with the written and spoken word. Naoki Sakai's argument can be broken down into three parts: A) An interpretation of Ito Jinsai's philosophy of nature and feeling, B) The relation of body and mind to ethics, and C) the inscriptional nature of virtue. Here, I will focus on A. 
According to Ito Jinsai, feeling is not the body or mind, but rather, a moving field that provides the environment through which the human body can interact with others. He explains that there exists a "transitiveness inherent in feeling," (90) it's not predetermined, it's fluent, which enables ethical action. In this moving field, there is a large degree of uncertainty, but your fears aren't exploited. This field of uncertainty becomes your opportunity to strengthen yourself and to develop a sociality in which you can tolerate the unknown, and feeling is the attitude you develop as you embrace the chaos. Feeling can neither be a limitation on the individual's state of being, but instead, a structure of linkage. He states, "Feeling, as I have emphasized, a structure of linkage, a linkage of the human body with the Other (92)." Feeling links your body to the world, which connects to the way that we interact with others. You can't attribute causality to feeling, since it serves as a link between the person and the environment.  
Ito Jinsai further explains the sociality of feeling through the nature of "Ai" or love. "Ai" is not supposed to be calculated, but it arises, and it comes into contact with the other. It is the emotion that links you to the Other. Just as feeling causes you to tolerate uncertainty, love is open with the unexpectedness of the other. Some may base the idea of love on the expectation of reciprocity, such as determining someone's "love" for them by quantifying the things that they do for them. However, to Ito Jinsai, this wouldn't be considered love. He characterizes Ai as "the difference in the channel of this transference in the rejection and abandonment of such a wish for homosocial complicity with others" (109). Since there is the abandonment of mutual exchange, love doesn't rely on transactions. This is attributed to the fact that engaging in transactional relationships would remove the unexpectedness of feeling.      
Feeling is a comportment or attitude in which you are open to things that you don't know, the ability to open yourself up with the other. According to Naoki Sakai, the openness of sociality is attributed to the fact that the self (Shutai) is detached from the movement of desire, or the subject (Shudai): "The shutai-self as a correlate of desire is affirmed because of its immediate linkage to the object of desire, whereas the shudai-self as thematically  reflected upon should be denied because it creates a rupture or nothingness that separates the self from the world" (92). Here, Naoki Sakai attests to the uncertainty, or unawareness of the subject while they are speaking. The Shutai is only the subject of the matter, you can't know what it is within itself, you can only know it as a trace. A person can never have complete control of what they're saying, or how others perceive their words. It's only until after the fact of the matter that they are able to reach that clarity. Feeling, in relation, is your ability to tolerate your vulnerability during this process. 
Ito Jinsai's philosophical approach to feeling will allow us to approach things with a certain humbless, which allows us to achieve a different mode of being. There is this idea that we have to be sovereign knowers, but that's not possible, since this attitude masks the inability to confront the unknown. By cultivating a comfort with ambiguity and approaching sociality with a sense of openness, we adopt a more nuanced and retrospective approach to life.  
Jules Joseph Lefebvre's piece Graziella (1878) represents the title character from a novel by Louis de Lamartine, which recounts the tragic love affair between a young French man and the beautiful daughter of a Neapolitan fisherman. The story ends with the heroine dying of grief after her lover was forced to abandon her. The artwork portrays Graziella sitting atop of Mount Venus and mending a fishing net as she gazes over the distant sea. The piece is currently on view at the Museum of Metropolitan Art in New York City, NY, in Gallery 800. The French painter used oil on canvas as his painting technique.
The fishing net, the red petals, and the smoldering profile of Mount Venus all come together to tell the story of Graziella and her tragic love affair. The fishing net gives the background information on Graziella, as she was the granddaughter of a fisherman. However, the fishing net serves another purpose for the painting. The fishing net is a "lure" for the painting, creating this path that the viewer follows, which leads the viewer to notice the red petals on the ground. The focus of the painting starts at the upper-half of the painting, which causes the viewer to focus on the red petals on Graziella's head, then use her body as a trace to navigate the rest of the painting. The red petals drifting from her hair and onto the ground portray the fading passion between her and her lover, foreshadowing her imminent collapse as her lover is forced to leave her. The bright red color of the petals contrasts the subdued tones dominating the rest of the painting, making a bold impression despite its small presence. This particular shade of red may serve to symbolize the youthful, passionate nature of the relationship. 
The subject's warm, muted skin tone creates a subtle blending effect that appears as if the subject is merging into the smoldering atmosphere of Mount Vesuvius, the main setting of Louis De Lamartine's novel. Lefebre's usage of the colors yellow, green, and gray, contributes to the overall haziness of the atmosphere. The smoldering profile of the background symbolizes the tumultuous emotions and uncertain futures faced by Graziella, as she grapples with the loss of her lover. In essence, the hazy, smoldering atmosphere foreshadows the couple's inevitable demise. 
Lefebvre uses more defined shadows for Grazeilla's face and implied lines for the rest of her body. The implied lines are used for Graziella's hair, creating a sense of motion. This causes the viewer to draw attention to Graziella's face, as well as the flowers on her head. The use of warm colors in the painting is used to create a sense of unity and variety in the painting. He uses this technique to make the artwork as realistic as possible while also conveying the aura of Lamartine's novel. Graziella's blue tunic contrasts the warm tones of the atmosphere as well as Graziella's hair. It evokes a sense of calmness, reflecting the tranquil thoughts in Graziella's mind as she sits atop Mount Vesuvius. The blue tunic not only reflects her state of mind, but it also shows her gentle and melancholic nature as a person. 
The light source is not explicitly shown in the painting, but based on the way that the light is cast on Graziella's face as well as the shadow of the rock, it can be implied that the light source is coming from the upper left hand corner of the painting. However, in this painting, the light source doesn't seem to be as imminent due to the murky color of the sky, so the shadows aren't as distinct as they would be if the sky was more clear. 
Lefebvre's use of colors and symbolism both go hand-in-hand when telling the story of Louis de Lamartine's novel. Despite not having read the novel, the viewer can still grasp a sense of who Graziella is and her inner thoughts. Even though a lover isn't explicitly seen in the painting, the viewer can see that the red petals symbolize romance and passion, thus implying a potential romantic partner. The artwork shows that a painting, using specific colors and symbols, can cause an audience to approach a character with depth in the same way that a novel would. 
In computer science, garbage collection refers to the automated collection of memory in a program or system. It is a process that includes the automatic reclamation of heap-allocated storage and the application never has to be free. It is common in many dynamic languages such as Python, Ruby, Java, Perl, ML, Lisp, and Mathematica. Variants of garbage collectors (conservative garbage collectors) exist for C and C++, however, they cannot necessarily collect all garbage. The purpose of garbage collection is to prevent memory leaks that could lead to inefficient memory usage and performance problems.
Memory managers rely on the usage of certain blocks, and rely on a set of conditionals to determine whether or not the blocks have an assigned pointer to them. There are many different kinds of algorithms that garbage collectors use in order to perform their tasks:
Mark and Sweep Collection (McCarthy, 1960): The mark-and-sweep algorithm is one of the earliest garbage collection algorithms, proposed by John McCarthy in 1960 and involves two main phases: marking and sweeping. The Mark phase starts off by traversing the entire memory space, starting from the root objects, then marks all reachable objects by setting a "mark" on each visited object. To perform this operation, a graph traversal is required. In the traversal, every object is considered the nodes, and the root is a variable that refers to an object and is supposed to be directly accessible by a local variable. This procedure then transitions to the second phase, also known as the "Sweep Phase." This algorithm sweeps through the entire memory space again, taking the unmarked objects into consideration. These unmarked objects are considered garbage, and their memory is deallocated. The marking process ensures that only the reachable objects remain unmarked. The mark-and-sweep algorithm is known as a tracing garbage collector because it traces the entire collection of objects that are directly or indirectly accessible to the program in two distinct parts. This Mark and Sweep algorithm is commonly used for many garbage collectors, as it is simple and effective at identifying garbage. It can also handle cyclic references, thus ensuring that the algorithm never ends up in an infinite loop. However, this algorithm may introduce pauses during the sweep phase, thus instilling fragmentation in the program. This means that memory is available in "fragments," but the space is unable to be utilized. Fragmentation can be preventable if the pieces of memory are compacted together to form one large block.
Reference Counting (Collins, 1960): Reference Counting is an algorithm proposed by John McCarthy and refined by Collins in 1960. It is a programming technique of storing the number of references, pointers, or handles to a resource, such as a block of memory. Reference counting can be used to deallocate objects that are no longer needed. When a reference object is created, the reference count is incremented.When a reference is removed or goes out of scope, the reference count is decremented. Then, the switch for garbage collection is triggered, and objects with a reference count of zero are considered garbage. The collector periodically or incrementally identifies and collects the objects with a reference count of zero. The benefit of using this method is the simplicity of the algorithm as well as the efficiency of short-lived objects. While the simplicity is beneficial, reference counting may be inefficient for cyclic references, adds overhead to reference manipulations, and requires additional memory for reference counts.
Copying Collection (Minsky, 1963) - A copying garbage collector utilizes two heaps, with the first one being the current working heap (from-space) and the second heap that needs to be in the memory during the garbage collection process (to-space). During garbage collection, the reachable objects are traced from the root set, then these objects are copied from the "from" space to the "to" space. The unreachable objects are left behind in the "from space." After copying the objects, the roles of the"from" and "to" spaces switch places,then the process repeats in the next garbage collection cycle. The benefit of using this method is that it is effective for short-lived objects, eliminates fragmentation, and improves cache locality. The downside is that it requires additional memory for the copying processes.
All three algorithms are the foundation for modern garbage collection techniques, and aid in releasing memory that is no longer in use. Modern garbage collectors often combine aspects of each of these algorithms to optimize efficiency and performance.
The main objective of "Lab 2: Computer Aided Design Competition" was to use the fundamentals of Computer Aided Design (CAD) and Fusion 360 to modify a poorly-designed part. The ultimate goal was to improve the safety factor of the poorly-designed part, and the competition was ranked by a design ratio that uses the difference of the initial and final safety factors as well as the volumes. The winning team was the one who had the highest ratio.
Our first trial had a safety factor of 0.19, our second trial had a safety factor of 0.37, and our last trial had a safety factor of 11.2 The last safety factor was used to calculate the competition ratio which was 31578.187 mm^3 and ended up placing second out of five teams. 
Computer Aided Design (CAD) is a software that is used to design 2D and 3D prototypes and aids in modifying and optimizing the design process. The flexibility of the software allows the user to readily access different tools they may need to design their prototype such as extrude, point, sketch, and cut. Its user-friendly interface also allows the project to be shared with others and reused multiple times. A disadvantage to using CAD is that it only autosaves every 10 minutes, so if the user doesn't manually save during the time in between work can be lost. Another disadvantage to using CAD is that to load the safety factor, you have to manually go to the settings and reload the entire project which takes up more time instead of the program automatically taking in the changes and speeding up the process. 
One of the main components that was used in the competition that is also a contributing factor to other designs is the ability to view the safety factor, which is the ratio of the total strength to the working stress of the prototype (EG 1004 Lab 2 Manual). The safety factor can be viewed before and after the mechanism is edited, which allows the user to view the difference and see where exactly the stress needs to be changed or removed. The safety factor is measured via the Mesh tool, which is a boundary along the prototype that consists of polygons that determine the precision. The safety factor varies depending on the type of material used to construct the prototype, and the materials that can be used to construct the figure were aluminum, steel, copper, and lead. Aluminum is a flexible material that is resistant to corrosion. Steel is malleable and has a high toughness, allowing a design to have structure as well as strength. Copper is a good conductor of electricity that is malleable but lacks in sturdiness. Compared to copper, lead is a poor conductor in electricity but is highly malleable and is resistant to corrosion.
Fusion 360 encompasses all the properties of CAD and puts it in a software for 3D modeling. There are 2 main workspaces in Fusion 360: Design and Simulation. The Design Workspace allows the user to create mechanical designs and edit and receive output of the geometrical constraints.  The Simulation Workspace allows the user to load their mechanical design into a system that outputs the safety factor and view the applied force in certain areas of the design with green being the safest area and red being the area where the most force is applied. 
The Fusion 360 software was used to reconstruct the design and obtain the competition factor. The main tools that were used in the lab were Extrude, Mesh, Simulation, and Loft.
Research on how the object was going to be redesigned required the internet and the different ideas were reconstructed on paper. The design represented a hinge, so research was conducted to view what an ideal hinge would look like and where the force would be applied. The original design was a wide base consisting of two rectangular blocks standing up. The two rectangular blocks were highlighted in red, meaning that the safety factor in that area is significantly lower than the rest of the design and the original material was lead. Also, the two arrows demonstrated in the image below show where and how the force is being applied, and the main force of the object is pushing towards the two rectangles. The initial safety factor of the design was .081 and the volume of the body was  27339.049 mm^3.
To fix this problem, several sketches were created to help support the two rectangular blocks in the Design Workspace. The final design consisted of drawing one rectangle each right below the hole of each block with a 6x5 dimension. The dimensions were acquired by zooming in on the structure to view the grid. Another square was drawn 12 units away from the bottom of each rectangle, and the two squares were connected together by using the Point tool to find the midpoints of each side of the square, and the Loft tool to connect the top and bottom squares via the midpoints. 
To acquire more support to the structure, two triangles were drawn on the inside of the connection between the rectangles and the base. The two triangles were drawn by finding the midpoint of the rectangle by using the Point tool and the midpoint of the block created via the Loft Tool. Once the two triangles were formed, with each one being at the edge of the design, the Extrude tool was used to extend the edges of the triangle to connect to each of the other ends of the blocks that were recently created. 
The original design of the structure was lead, which caused complications with the design because lead is a weaker substance that does not hold prolonged duration. To fix this problem, the material was changed to aluminum to acquire a sturdier structure. To change the material of the design, the Study Materials was right clicked via the Browser tab. 
Once the design was completed, to run the analysis, the "Simulation" Workspace was then used by going to the Mesh tab and "Generate Mesh" from there and selecting the redesigned part. The "Solve" button was clicked at the bottom right of the screen that popped up and the final safety factor of the prototype was calculated, with new shading of the design to demonstrate the safety of the forces that are now applied to the object. The final volume was also taken note of along with the final safety factor of the design.
The issue with the first design was that the two bars connecting the rectangle to the base did not have enough support, and with the overall force pushing towards the rectangles standing up, most of the stress was towards the parts connecting from the top to the bottom. The material was also made of lead, which caused the overall structure to be significantly weaker and less durable. 
The second and final simulation consisted of adding the two triangular blocks to the inside of the connection of the blocks to add more support and the material of the design was changed to aluminum to add sturdiness.
The entire design is now blue, meaning that according to the bar in the bottom right the entire design has a safety factor of eight or greater. The final safety factor was 11.2 and the design had a volume of 31578.187 mm^3. The final competition ratio was then calculated using the initial and final safety factors as well as the volumes of both designs. The final competition ratio was calculated to be 3.517 and resulted in second place.
Compared to the previous trials, the final trial not only consisted of changing the design of the structure but the material as well. The material of the design was the main contributing factor to the sturdiness, as it was significantly weaker before the material was changed. The final volume of the design was no more than double the original design, as the original design had a volume of 27339.049 mm^3 and the final volume of the design was 31578.187 mm^3, making the final volume of the design only 1.15 times greater. Compared to the other teams, the outcome for this lab resulted in the third-lowest safety factor. Despite this outcome, the final volume, 3.154 mm^3 was the lowest out of all four teams, which was optimal for the final calculations of the competition ratio which not only factored in the final safety factor but the final volume as well.  The original plan was to provide support to the two rectangles standing up since most of the force was applied there. While providing support to the two rectangles was attempted, the applied force did not have support, causing the first trial of the design to be inefficient since the force was allocated to the part of the design that was edited. To fix this problem, more support was added to the base of the design by generating the triangular blocks.  These triangular blocks allowed the connection to have more support while the changing the structure's material to aluminum helped distribute the safety factor of the design evenly. 
The redesign of the structure was functional, as all areas were evenly distributed in terms of tension and force. All areas of the structure were blue, indicating that the safety factor in all areas of the design was eight or greater which exceeded the requirements for the competition. While the design was functional, improvements could have been made for the design. While the design was functional for the competition, its practicality is limited as the final redesign of the structure can result in stiffness and inflexibility. The triangles that were included in the final design have the potential to be fragile and easily fall apart when built in a real structure, such as a hinge or a bridge. To improve this part of the design, the beams connecting the rectangles to the base can be steeper, and instead of triangles, the entire space can be filled with material for more stability. 
Overall, in order for a prototype to become successful and exceed the initial safety factor, the forces of the prototype need to be distributed evenly so the safety factor is distributed evenly as well. The material of the design also plays a large contributing factor to the safety factor as well, as different materials have different malleability and efficiency. To even out the force, it needs to be pushed against the arrows so the state of the design can reach equilibrium. 
I aided in coming up with the design for the second trial and aided in thickening the extrusions of the triangular blocks to add more sturdiness. I was also responsible for keeping track of all the data for our group, such as the volume, safety factor, and amount of force applied.  I was also the one who created the final calculations for each of the trials, with the final one being the competition factor that won second place. I kept track of all the different tools required to execute the procedure and drew the sketch ideas for our group. I aided in helping my group understand the fundamentals of the CAD software as well as locate the tools required to complete the project. 
The objective of Lab 8 "Processes & Water Filters" was to design and create a water filter which is efficient and cheap to make contaminated water samples into clean water. A process flow diagram was created by using the water treatment system guideline and physical filter design and help design the water treatment system. Each group was judged by a competition ratio which took the mass of filtered water, change in water turbidity, cost, and flow rate into account. The group with the highest ratio won the competition.
Only one trial was conducted. The final competition ratio was 1.94. The water filter design placed third out of four teams. The first-place water filter design had a competition ratio of 7.43, the second-place water filter design had a ratio of 2.79, and the fourth-place design had a ratio of -1.86.
Currently, water scarcity affects more than 40% of the world's population (NYU Tandon, 2023). Obtaining drinking water from freshwater sources and unconventional water sources is currently the main way for engineers to solve the problem of water scarcity. Due to the contamination in these water resources and the high standards of drinking water, the removal of contaminants through filtration and related treatment methods is now a necessary water treatment process.
In daily life, the water treatment process is divided into five main steps. Figure 1 (NYU Tandon, 2023) shows the sequence of these five steps.
The initial stage of water treatment involves coagulation and flocculation. By adding positively charged chemicals, known as flocculants, it causes a chemical reaction to neutralize the negative charge of the contaminants in the water, which combine and form larger particles. In this lab, aluminum sulfate was used as a flocculant. The second step is sedimentation. Since flocs have a higher density, they will eventually settle to the bottom of the solution. Filtration, the third step in the water treatment process, removes floc from the water. In this stage, filters consisting of sand beds, gravel or other materials remove contaminants of different sizes through different pore sizes. In this lab, sand, gravel and activated carbon were used as filters to filter the water. Disinfection is the next step after filtration. Bacteria, viruses, parasites, etc. are killed in this process. Chlorine is usually used at this stage to treat the water and remove any chemical contaminants from it. Finally, the water is stored in the tank for later use. In this lab, only three steps including coagulation and flocculation, sedimentation, and filtration were used.
In order to ensure that the vessels and pipes in the water treatment system do not corrode and to ensure human health, the acidity and alkalinity of the water needs to be controlled and neutralized. pH is used to measure the acidity of water and it is based on a logarithmic scale of hydrogen ion concentration (1).
According to the United States Environmental Protection Agency (USEPA), water with a turbidity of 1.0 NTU can be considered as drinking water (NYU  Tandon, 2023).
To approximate the amount of water retained after filtration, engineers measure the mass balance of the system. Mass balance refers to the measure of the total mass flow that enters and exits a system during a chemical or physical process. The law of conservation of mass is important when drawing the Process Flow Diagram, which means that in a closed system, the mass is always constant over time. Figure 4 (NYU Tandon, 2023) shows the conservation of mass in the system at a certain stage.
The mass of the system input must equal the mass of the output. And in this process, chemical reactions may occur within the system, then the generated mass must be equal to the consumption of the system.
Process flow diagrams (PFD) are utilized in various engineering domains to demonstrate the sequential movement and processing of materials across several processing units and equipment. PFD monitors the flow and makeup of water as it passes through a filtration system. The elements including process vessels and equipment, process and utility flow lines, full energy and material balance, composition of every stream, and bypass and recycle streams must be included when sketching PFDs. Figure 5 (NYU Tandon, 2023) shows various elements of the system in a PFD.
Process vessels and equipment, such as Mixer 1, Mixer 2, Strainer, Trash, and Storage in Figure 5, change the physical properties of substances when they pass through. The process and utility flow lines are indicated by arrows. These arrows point out the direction of material flow and the composition of each flow. Full energy and material balance means that the system obeys the principles of energy conservation and mass conservation. They are usually written above the process flow lines. The composition of the flow is usually listed under the flow lines, and indicates the percentage of each material in each flow. This can be calculated by the equation (3).
The materials that were used to design the water filtration system were a computer with internet access, pH strips, contaminated water, an erlenmeyer flask, a funnel, a 500 mL beaker, a scale, 0.5 g of alum, 0.3 g of sodium bicarbonate, a glass stirring rod, two tsp of activated carbon, and one tsp of sand. To measure the water quality a turbidity sensor was used. 
To represent the percent of mass lost and the quantity of material, an excel sheet was utilized to visualize the relationship. The blank scatter plot was created via excel by selecting the insert "scatter" option in the "charts groups." The empty graph was right-clicked and the "select data" tab was selected. Once the "select data source" dialog box appeared, in order to add a new dataset to the graph, the "add" option was selected. The series name that was entered in this blank section was "% mass lost vs. teaspoons of sand." This process created the title of the graph. The "x" values of the graph were selected by highlighting the number of teaspoons of the material (sand, gravel, or activated carbon) and inputting them into the "series x" tab. The "y" values of the graph were selected by highlighting the "% mass lost" data and inputting those values into series y. To rename the chart axes and titles, the "chart design" tab was selected, then the "add chart element." After the "add chart element" tab was selected, the "Title" option was selected in order to add titles for each axis. A trendline for the graph was inserted by selecting the "trendline" option, then selecting "linear." To create the equation for the trendline, the trendline was double-clicked, then the "display equation" option was checked off. Since this lab had three materials, three different graphs were created for each material.
The empty beaker was weighed and its mass was recorded in the Excel spreadsheet. After recording the weight of the beaker, approximately 200 mL of warm contaminated water was added to the beaker. A turbidity sensor was then used to measure the water quality, which was 0.75. The beaker with the contaminated water was then reweighed to measure the mass of the contaminated water which was 218.49 grams. After the turbidity and the mass of contaminated water was measured, a ph strip was immersed in the contaminated water sample. The final color of the ph strip was compared to the pH color chart and measured as 6.0. 
After the initial data was collected, the ph was neutralized by adding 0.3 g of sodium bicarbonate (baking soda) to the contaminated water. Once the baking soda was completely dissolved in the mixture, another ph strip was immersed to remeasure the ph. The final pH of the mixture was 7.0, which indicates that the pH was neutralized and meets the USEPA standards.
The beaker of neutralized water was placed on the scale, and the scale was tarred after the beaker was placed onto the scale. 0.5 g of alum was added to the beaker of neutralized water, and the mass of the alum was recorded in the data sheet. After the alum was added, the contaminated water was mixed for ten seconds. Once the solution was mixed, the contaminated water rested for 10-15 minutes in order for flocculation to occur. 
Teju Cole's New York Times article "Getting Others Right" explains how paraphrasing the personal experiences of others can skew our ability to empathize with those around us. Cole argues that taking on the identity of others and appropriating what is theirs doesn't satisfy the moral responsibility of capturing the truth. He contrasts different works of photography to express his contention. Cole begins by criticizing Curtis's portraits and Nelson's project "Before They Pass Away" and states that the photos place the subject in an "anthropological past" by portraying the subjects in their tribal attire. He claims that this process of photography was ineffective, and that by displaying the Native Americans as their character in history, the story that was intended to be told through these photos ceased to exist. As a solution, rather than approaching the narrative with the preconceived notion of the identity of the individual, Cole proposes, "Telling the stories in which we are complicit outsiders has to be done with imagination and skepticism. It might require us not to give up our freedom, but to prioritize justice over freedom" (Cole). He praises the effectiveness of Daniela Zalcman's "Signs of Your Identity," as she allowed the subjects to visualize their own memories rather than platforming her own voice. Her work consisted of double exposures that joined the portraits of the subject and their memories by overlaying the images of places with portraits of people (Cole). Cole states that this method was successful, "she manages to accomplish quietly forceful reportage from material that could easily have been sensationalized" (Cole). As one views Zalcman's work, unlike Curtis and Nelson, one can understand that the intention of the photo was to tell the story of an individual, one that has vivid memories of the experiences that have shaped their identity. Understanding the experiences of others, especially those that haven't been experienced by themselves, is notoriously difficult, as it requires people to analyze brand new information that they haven't worked with before. 
Cole further explains this idea by stating that telling the story of others isn't a simple process that can be done through the lens of a single picture, or even a collection of photos. Stepping in the shoes of someone else requires the ability to express sympathy, and expressing sympathy is a complicated process. According to Cole, capturing the truth requires a multitude of factors such as "intuition, scrupulous context and moral intelligence," all of which a conscience can convey, but an inanimate object without a mind of its own would struggle to recognize. A photo doesn't have a conscience, as it can only capture appearances, and to Cole, this "bare fact" is particularly dangerous when it comes to taking on the struggles of others as one's own, "capturing how things look fools us into thinking that we've captured their truth. Unalloyed, it is worse than nothing" (Cole).  
If telling the story of others through a series of photos is "worse than nothing," how do these photos elicit such a strong emotional response, and why are we compelled to take some sort of action once we see them? John Berger's "Photos of Agony" uses McCullin's photography of victims during the Vietnam War as an example to explain this phenomenon. Berger represents the reaction of viewing these photos as a discontinuity within the viewer's experience of looking at the photo, and this "discontinuity" parallels the victim's discontinuity within the flow of time. As one views McCullin's work, Berger explains that we are seized by the picture, or "arrested," for a brief time. The viewer is engulfed by the picture and takes on the suffering of the subject as their own, "and as soon as this happens even his sense of shock is dispersed: his own moral inadequacy may now shock him as much as the crimes being committed in the war" (Berger). 
This act of sympathy, Berger suggests, leads to no purpose. Viewers might desire to initiate change based on their feelings in the moment, but converting those emotions into actions is easier said than done, resulting in a "hopelessly inadequate response to what we have just seen" (Berger). Berger claims that once the photo elicits an emotion reaction  from the viewer two possibilities may happen: "Either he shrugs off this sense of inadequacy as being only too familiar, or else he thinks of performing a kind of penance - of which the purest example would be to make a contribution to oxfam or to Unicef" (Berger). The viewer may either dismiss the photo or find a way to contribute out of a sense of moral inadequacy. In both situations, the issue becomes depoliticized and the response completely strips away the effect the photo was intended to have. 
Rather than using violent photography as a confrontation to garner a public response, Berger advises that we confront our own lack of political freedom instead, "In the political systems as they exist, we have no legal opportunity of effectively influencing the conduct of wars waged in our name" (Berger). Although we might feel compelled to take action once we see pictures of injustice, the action we take doesn't isn't nearly enough to resolve the overall crisis since it doesn't focus on the underlying meaning behind why today's political systems are structured the way that they are. Instead, the photo gets dismissed as shock value, and the viewers carry on while injustice still continues. 
Both Cole and Berger share the same objective of serving justice. The difference lies in the process: Cole focuses on utilizing sympathy, while Berger directly focuses on the political action. Cole claims that using photography as a means to serve justice can skew our ability to sympathize, while Berger argues that this very sympathy is the reason why no action is taking place. While both explore the way that photography shapes our understanding of the world, they approach the topic from different angles: Cole urges us to be more mindful of our own bias, on the other hand, Berger is more concerned with the aftermath of viewing such photographs. To effectively serve justice, one must use their platform to amplify the voice of others and be conscious of unjust political systems that are the backbone of our society. By doing so, we fulfill our moral responsibility of "capturing the truth." 
Once we capture the truth, how can we use our emotional reactions to bring out meaningful change? Frans De Waal, in his essay, "The Evolution of Empathy" provides a new perspective on the role of empathy. Sometimes, taking action can also mean saying or doing nothing at all. Prison guards were ordered to provide their prisoners with only bread and water, but one guard went against this command by occasionally sneaking in food for his prisoners. Although it was a small act, the prisoners remembered it as a sign that not all of their enemies were cruel. Many other events preceded this act, such as instances when soldiers had the opportunity to escape the negative repercussions of killing their captives but chose not to do so (De Waal). These soldiers exhibited empathy by restraining themselves from killing their enemies despite having the freedom to do so. De Waal's example parallels Cole's explanation of prioritizing justice over freedom. In Cole's discussion, the photographers had the freedom of expressing their subjects in the way that they wanted to, which in turn harmed the subjects By mentioning this event, De Waal substantiates Cole's claim about prioritizing justice over freedom: freedom is the ability to act in the way that one desires while being free of the consequences, but justice relies on how others feel when solving moral dilemmas (De Waal). 
In today's political climate, sympathy is not enough to resolve ongoing crises. In order for everyone to live a sustainable life without injustice, we think beyond what we feel. "Getting others right," capturing the truth, and serving justice is a multi-layered process that requires advanced thinking and a combination of both logical thinking and empathy. it's not about prioritizing one over the other but incorporating them both equally to address the issue at hand. 
Whenever I visit my grandparent's house, I am met with a plethora of photo albums. Most of these photo albums, if not all, contain images of my childhood. However, there are a few photo albums that show images of my parent's childhood. Some of these photos surprised me, as I saw my mother as a child sitting at the very edge, next to all of her other relatives. It was then that I realized that I couldn't imagine my mother as the same age as me at the time, who had similar experiences as my back in the 1970s. When I was struck with this realization, I couldn't help but wonder if my view of my parents would have been different if I hadn't seen those photos.
Teju Cole's "Memories of Things Unseen" illustrates the paradoxical nature of photography and its impact on the memory of ongoing experiences. Cole argues that photography can be used in two distinct ways, which are opposite in nature. On the one hand, photography can be used positively to construct a shared memory of humans. On the other hand, he states that it can be a malicious tool to control people's actions, "The government retains our images in order to fight terrorism, and corporations harvest everything they can about us in order to sell us things" (Cole).  He explains that photography's disruptive counterpart can be utilized as a tool of manipulation and an invasion of the user's privacy.  The constant act of photographing ourselves can be a source of discomfort, as these images have the potential to exist beyond our lifetimes and endure indefinitely. Consequently, the spontaneous nature of captured moments is lost, and they become permanent. Instead of providing a wider access to an eternal existence, removing the present moment through photography completely eliminates the concept of eternity. This paradoxical nature lies in the fact that when all things are considered eternal, nothing can truly be eternal.
Cole describes photography as a memorial to a memory, "Photography is a memorial art. It selects a moment that is to be remembered, with the moments before and after falling away like sheer cliffs" (Cole). By describing photography in this manner, he believes that photography is a hindrance to the memory, as it forces the individual to reminisce on the moments in that photo rather than incorporate experiences into their memories as a whole. 
He further explains this concept through the dinner party conversation about the application Snapchat, which allows the user to send temporary photographs to others around them. Unlike other applications, the duration of the photo only lasts for a brief period of time before it ceases to exist. Cole sheds light on the sentimental value of being able to preserve your real self in the moment while the photography vanishes, unlike a sequence where "the image lives on and the model is irretrievable" (Cole). 
Cole concludes that this rampant desire to store our memories is now focused on the lens of a camera. We utilize photography as a way to exercise our fixation with the passage of time that has faded from human memory. This hyperfixation on the encapsulation of memories has served us globally, yet this act also has the potential to cause a security breach without the person's knowledge.
Just as Cole argues that photography is a hindrance to human interaction, Susan Sontag's "On Photography" hones in on the same concept. Photography, according to Sontag, is a form of surveillance. Her contention goes hand–in-hand with Cole's "memorial to a memory," as she claims that photography is a way of "certifying experience." This "certification," as Sontag claims, lulls the individual into a sense of security since it obliterates any obscurity regarding their experiences. She argues that photography serves as the closure to unfamiliarity, and brings in the concept of travel to substantiate her point. When someone travels to a place that they have never been before, rather than experiencing the moment, the person takes a photograph to capture the moment, or in this case, the "unfamiliarity." She infers that the person takes the photo out of an unconscious fear that they would never visit the place again, or have the same experience that they did the first time they traveled. By taking a photo, the person has the ability to reminisce on the memory and provides assurance on the validity of their experience. This sense of comfort, Sontag claims, transcends into a compulsive need to retain that state of mind. The constant desire to find an answer, to provide closure on an incomplete memory, is satisfied through photography. Eliminating the openness to memory causes people to become more dependent on photography rather than their own thoughts and experiences. 
Both Cole and Sontag would agree that photography freezes time and is a physical manifestation of the concept of memory. They both establish memory as an abstraction that is impossible to fully comprehend in a physical form. Memory, to both Cole and Sontag, is an intuitive response, one that can't be fully complete even with photography in place. Memory provides openness and ambiguity, while photography is set and stone. The person's recollection of a memory is blinded by the photograph, as the person recalls the memory just by looking at what is presented in the photo. By solely relying on memory, the person is able to focus on the experience and the emotional connection that they have to that experience. 
By becoming dependent on photography, we become less dependent on memory, which in turn fosters a disconnection between the past and the present. If society is becoming increasingly dependent on photography as a means to memorialize their past experiences, then how can we express familiarity with the others around us without it?  
In an article by Samira Dodson called "Detecting valence from unidentified images," there is a link between positivity and familiarity without concrete identification. Five studies were conducted to find the correlation between familiarity and emotion, with the findings showing that the feelings of positivity and familiarity are closely related to each other. The basis of the research was based on a phenomenon known as "recognition without identification," which solely relies on memory and the emotional connection one has with that memory. The study involved providing a group of individuals with a list of words to memorize, and subsequently showing them word fragments to make it difficult to recognize and comprehend the meaning of the words. The finding from this experiment was that participants who were unable to identify the word, they can differentiate between old and new test words to fill in the blanks for each letter. This demonstrates that the concept of memory is not entirely conceptually driven, as the familiarity of the words triumphs the conceptual connection to the word itself. This study was conducted with various types of stimuli. In another experiment, photography varying in threat level was utilized as the source of the test subject's emotional response. The test subjects were then asked to rate the source of familiarity on a scale of 1 to 10. The finding of this experiment was that the threatening images were rated as more familiar than the non-threatening ones. This experiment concluded that the feelings of positivity and familiarity had a bidirectional relationship, as  positivity was misattributed to the increase of familiarity (Dodson). 
Contrasting Sontag and Cole, the research that was conducted showed that memory isn't entirely abstract, as it still relies on physical objects to invoke a sense of familiarity. While photography has the potential to be a hindrance to memory, it is also used to invoke a sense of familiarity between the photograph and the viewer. This relationship demonstrates that memory still relies on physical things, not just emotion and experience. This sense of familiarity one feels when the photograph is seen is the aftermath of obtaining a memory, and this familiarity is used to foster relationships with others around us. This memory in its physical and conceptual form connects us from the past to the present, and helps us understand history. Even though society might become increasingly dependent on photography, photography goes hand-in-hand with creating memories and understanding our past. Obtaining a physical manifestation of the past helps people better understand what needs to be done to change our future. 
